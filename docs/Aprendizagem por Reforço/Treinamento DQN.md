# DocumentaÃ§Ã£o do Treinamento DQNRouter

Este documento descreve a arquitetura e o funcionamento do **sistema de treinamento do DQNRouter**, implementado no arquivo `train_DQNRouter.py`.

---

## ğŸ“Œ VisÃ£o Geral

O cÃ³digo implementa uma arquitetura **Actor-Learner distribuÃ­da**, inspirada em algoritmos como **Ape-X DQN**.  
A ideia central Ã© separar o processo de **coleta de experiÃªncias** (atores) do processo de **aprendizado da rede neural** (learner).  

### Componentes principais:
- **Actor (Atores):** executam episÃ³dios da simulaÃ§Ã£o, interagem com o ambiente e produzem experiÃªncias.
- **Learner (Aprendiz):** atualiza a rede neural (policy e target networks) com base nas experiÃªncias recebidas.
- **Logger:** registra mÃ©tricas de desempenho de cada episÃ³dio.
- **Fila de ExperiÃªncias:** conecta atores ao learner.
- **Fila de SaÃ­da:** conecta atores ao logger.

---

## âš™ï¸ Fluxo do Treinamento

1. **Carregamento do modelo base**
   - O estado inicial do sistema ferroviÃ¡rio Ã© carregado de um arquivo `.dill`.
   - Ã‰ construÃ­do o espaÃ§o de **estados** e o espaÃ§o de **aÃ§Ãµes**.

2. **ExecuÃ§Ã£o de EpisÃ³dios (Actor)**
   - Cada ator roda uma simulaÃ§Ã£o de 30 dias:
     - Observa o estado.
     - Escolhe uma aÃ§Ã£o (via DQN ou aleatÃ³ria, conforme polÃ­tica).
     - Recebe a recompensa e transiÃ§Ã£o.
   - As transiÃ§Ãµes sÃ£o enviadas para o **Learner** via `experience_queue`.
   - EstatÃ­sticas (volume operado, demanda atendida) sÃ£o enviadas para `output_queue`.

3. **AtualizaÃ§Ã£o da Rede (Learner)**
   - O **Learner** roda em processo separado.
   - Ele consome experiÃªncias da `experience_queue` e aplica:
     ```python
     learner.update(experience)
     ```
   - Periodicamente salva os pesos das redes (`policy_net` e `target_net`).

4. **Registro dos Resultados (Logger)**
   - O **Logger** consome dados da `output_queue`.
   - Gera arquivos `.log` com mÃ©tricas por episÃ³dio.

5. **Loop Principal**
   - Executa `TRAINING_STEPS`.
   - Em cada passo:
     - LanÃ§a processos de **atores**.
     - Espera terminarem.
     - Solicita ao **Learner** salvar os modelos atualizados.
   - Ao final, todos os processos sÃ£o encerrados.

---

## ğŸ§© Diagrama da Arquitetura

### Arquitetura Geral

```mermaid
flowchart TD
    A[Actor Processos de EpisÃ³dios] -->|ExperiÃªncias| B[Experience Queue]
    B --> C[Learner]
    C -->|Atualiza Redes (Policy/Target)| D[(Modelos Salvos)]

    A -->|MÃ©tricas| E[Output Queue]
    E --> F[Logger]
    F --> G[(Arquivos de Log)]
```

## Detalhe do Loop de Treinamento

```mermaid
sequenceDiagram
    
    participant Main
    participant Actor
    participant Learner
    participant Logger

    Main -> Actor: Inicia episÃ³dios
    Actor->>Learner: Envia experiÃªncias (s, a, r, s')
    Actor->>Logger: Envia mÃ©tricas (volume, demanda)
    Learner->>Learner: Atualiza policy_net e target_net
    Main->>Learner: Solicita salvamento do modelo
    Logger->>Logger: Registra logs em arquivo


```

## ğŸ“‚ Estrutura de Processos

* Main Process
  * Cria e coordena subprocessos.

* Actor Processes
  * Rodam simulaÃ§Ãµes independentes.

* Learner Process
  * Consome experiÃªncias e treina a rede.

* Logger Process
  * Armazena logs dos episÃ³dios.

## ğŸ”‘ BenefÃ­cios da Arquitetura

* SeparaÃ§Ã£o clara de responsabilidades:

  * Atores = coleta de dados.
  * Learner = atualizaÃ§Ã£o de rede.

* Escalabilidade:

  * FÃ¡cil aumentar o nÃºmero de atores (paralelismo).

* PersistÃªncia:

  * Modelos sÃ£o salvos periodicamente.
  * Experimentos sÃ£o registrados em logs.

## ğŸ“Š Resumo

Essa implementaÃ§Ã£o do DQNRouter Ã© um sistema distribuÃ­do de aprendizado por reforÃ§o com arquitetura Actor-Learner.
Os episÃ³dios sÃ£o gerados em paralelo, experiÃªncias sÃ£o enviadas para o Learner, e os resultados sÃ£o logados para anÃ¡lise futura.