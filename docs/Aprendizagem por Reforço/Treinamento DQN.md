# Documenta√ß√£o do Treinamento DQNRouter

Este documento descreve a arquitetura e o funcionamento do **sistema de treinamento do DQNRouter**, implementado no arquivo `train_DQNRouter.py`.

---

## üìå Vis√£o Geral

O c√≥digo implementa uma arquitetura **Actor-Learner distribu√≠da**, inspirada em algoritmos como **Ape-X DQN**.  
A ideia central √© separar o processo de **coleta de experi√™ncias** (atores) do processo de **aprendizado da rede neural** (learner).  

### Componentes principais:
- **Actor (Atores):** executam epis√≥dios da simula√ß√£o, interagem com o ambiente e produzem experi√™ncias.
- **Learner (Aprendiz):** atualiza a rede neural (policy e target networks) com base nas experi√™ncias recebidas.
- **Logger:** registra m√©tricas de desempenho de cada epis√≥dio.
- **Experience Queue:** conecta atores ao learner.
- **Statistics Queue:** conecta atores ao logger.


![alt text](../images/actor_learner.png)
---

## ‚öôÔ∏è Fluxo do Treinamento

1. **Carregamento do modelo base**
   - O estado inicial do sistema ferrovi√°rio √© carregado de um arquivo `.dill`.
   - √â constru√≠do o espa√ßo de **estados** e o espa√ßo de **a√ß√µes**.

2. **Execu√ß√£o de Epis√≥dios (Actor)**
   - Cada ator roda uma simula√ß√£o de 30 dias:
     - Observa o estado.
     - Escolhe uma a√ß√£o (via DQN ou aleat√≥ria, conforme pol√≠tica).
     - Recebe a recompensa e transi√ß√£o.
   - As transi√ß√µes s√£o enviadas para o **Learner** via `experience_queue`.
   - Estat√≠sticas (volume operado, demanda atendida) s√£o enviadas para `output_queue`.

3. **Atualiza√ß√£o da Rede (Learner)**
   - O **Learner** roda em processo separado.
   - Ele consome experi√™ncias da `experience_queue` e aplica:
     ```python
     learner.update(experience)
     ```
   - Periodicamente salva os pesos das redes (`policy_net` e `target_net`).

4. **Registro dos Resultados (Logger)**
   - O **Logger** consome dados da `output_queue`.
   - Gera arquivos `.log` com m√©tricas por epis√≥dio.

5. **Loop Principal**
   - Executa `TRAINING_STEPS`.
   - Em cada passo:
     - Lan√ßa processos de **atores**.
     - Espera terminarem.
     - Solicita ao **Learner** salvar os modelos atualizados.
   - Ao final, todos os processos s√£o encerrados.

---

## Detalhes de Implementa√ß√£o

### Simula√ß√£o e Cria√ß√£o das Transi√ß√µes

O fluxo b√°sico de simula√ß√£o de um trem √© descrido na figura abaixo. Os eventos principais s√£o:

* Chegada do trem a um determinado n√≥ da ferrovia (terminais de carregamento/descarregamento);
* In√≠cio do processamento (carregamento ou descarregamento);
* Fim do processamento;
* Libera√ß√£o do trem;
* Envio para a pr√≥xima esta√ß√£o. Caso n√£o existe uma pr√≥xima esta√ß√£o, o roteador (agente tomador de decis√£o) decide para qual fluxo enviar o trem.

![alt text](../images/diagrama_simulacao.svg)

O disparo dos eventos √© gerenciado pelo simulador, e √© respons√°vel por alterar o estado do sistema. Por esse motivo, os **roteadores baseados em mem√≥ria** (que implementam Q-learning ou DQN, por exemplo), precisam que o simulador utilize um evento que execute uma rotina de captura do estado anterior (s) e posterior (s') ao evento (a), al√©m de salvar essa transi√ß√£o (s, a, s') em uma mem√≥ria. Para implementar esse comportamento, utilizamos o padr√£o de projeto Decorator, atrav√©s da classe `DecoratedEventFactory`. Sua implementa√ß√£o base √© apresentada a seguir:

```python
class DecoratedEventFactory(EventFactory):
    def __init__(self, pos_method: Callable, pre_method: Callable):
        self.pos_method = pos_method
        self.pre_method = pre_method

    def wrapper(self, callback: Callable):
        def decorated(*args, **kwargs):
            self.pre_method(*args, **kwargs)
            try:
                callback(*args, **kwargs)
                self.pos_method(*args, **kwargs)
            except FinishedTravelException as e:
                kwargs['event_name'] = 'ROUTING'
                info(f"Taking a snapshot because event generate an exception: {e}")
                self.pos_method(*args, **kwargs)
                raise e
        return decorated
    
    def create(self, time_until_happen, callback, data):
      decorated_callback = self.wrapper(callback)
      return Event(
          time_until_happen,
          decorated_callback,
          data,
          event_name=f"Decorated {callback.__qualname__}"
      )
```

Nessa classe vemos o m√©todo `wrapper`, que recebe a callback do evento. Antes de executar a callback ela executa um `pre_method()`, e ap√≥s executa um `pos_method()`. √âsses m√©todos s√£o injetados em DecoratedEventFactory e encapsulam a l√≥gica desejada. No caso, o comportamento que queremos encapsular est√° implementado na classe RailroadEvolutionMemory, onde o `save_previous_state` √© o `pre_method` e `save_consequence` √© o `pos_method`.


```python
class RailroadEvolutionMemory(AbstractSubject):
    def save_previous_state(self, *args, **kwargs):
        state = self.take_a_snapshot(is_initial=kwargs.get('is_initial', False))
        self.previous_state = state

    def save_consequence(self, *args, **kwargs):
      event_name = kwargs.get("event_name", "AUTOMATIC")
      next_state = self.take_a_snapshot(*args, **kwargs)
      self.save(
          s1=self.previous_state,
          s2=next_state,
          a=event_name,
          r=next_state.reward(),
      )

    def take_a_snapshot(self, *args, **kwargs) -> TFRState:
        is_initial = kwargs.get('is_initial', False)
        if not self.railroad:
            critical("Memory does not know the railroad and therefore does not perform any snapshots")
            return
        state = self.state_factory(railroad=self.railroad, is_initial=is_initial)
        return state
```

O m√©todo `take_a_snapshot` utiliza uma f√°brica de estados (`self.state_factory`)para converter o modelo da ferrovia em um estado compreendido pelo algoritmo de aprendizagem. 


### Observa√ß√£o de Estados

A classe RailroadEvolutionMemory √© repons√°vel por capturar os estados antes e deopis da execu√ß√£o dos eventos (ou seja, as experi√™ncias da simula√ß√£o) e armazen√°-los em um vetor `.memory`. J√° a classe `ExperienceProducer` √© respons√°vel por observar a atualiza√ß√£o dessa mem√≥ria e escrever as esperi√™ncias na fila `ExperienceQueue`. Esse comportamento foi implementasdo seguindo o padr√£o de projeto Observer, tamb√©m conhecido como PubSub. A figura abaixo apresenta a estrutura base desse padr√£o, conforme documentado no [Refactoring Guru](https://refactoring.guru/design-patterns/observer). Para entender mais sobre a implementa√ß√£o desse padr√£o, acesse [Observadores e Sujeitos](<../Conceitos importantes/observers_and_subjects.md>).


![alt text](../images/pub_sub.png)


### Aprendizado

Sempre que uma nova experi√™ncia √© adicionada na `ExperienceQueue`, o processo respons√°vel pela aprendizagem do roteador executa o m√©todo `learner.update()`, que implementa o algoritmo de aprendizado do DQN (Deep Q-learning Network) utilizando a biblioteca **TensorFlow**. Esse m√©todo funciona da seguinte forma:

```
1. Sele√ß√£o de 15 experi√™ncias (s, a, s', r) aleat√≥rias.
2. Converter estados, a√ß√µes e recopensas em tensores.
3. Para cada "pr√≥ximo estado", identificar as melhores "pr√≥ximas a√ß√µes" de acordo com a pol√≠tica atual.
4. Calcula o valor `Q` do pr√≥ximo estado tomando a melhor "pr√≥xima a√ß√£o" de acordo com a rede alvo.
5. Calcula o valor de `Q_bellman` para a recompensa atual e o valor de `Q` do pr√≥ximo estado, conforme a equa√ß√£o de bellman:
  5.1 Equa√ß√£o de bellman: Q* = R + GAMMA * Q'
  5.2 Caso o pr√≥ximo estado seja um estado terminal (ou seja, termina a simula√ß√£o), Q* = R
6. Calcula o valor de `Q_rede` com base na rede atual
7. Compara o `Q_rede` obtido pela rede e o `Q_bellman` obtido pela equa√ß√£o de bellman e encontra a fun√ß√£o de perda (loss function).
8. Ajusta os pesos da rede atrav√©s de otimiza√ß√£o com gradiente decendente e a loss function

```

---



## üìÇ Estrutura de Processos

* Main Process: Cria e coordena subprocessos.

* Actor Processes: Rodam simula√ß√µes independentes.

* Learner Process: Consome experi√™ncias e treina a rede.

* Logger Process: Armazena logs dos epis√≥dios.

## üîë Benef√≠cios da Arquitetura

* Separa√ß√£o clara de responsabilidades:

    * Atores = coleta de dados.
    * Learner = atualiza√ß√£o de rede.

* Escalabilidade:

    * F√°cil aumentar o n√∫mero de atores (paralelismo).

* Persist√™ncia:

    * Modelos s√£o salvos periodicamente.
    * Experimentos s√£o registrados em logs.

## üìä Resumo

Essa implementa√ß√£o do DQNRouter √© um sistema distribu√≠do de aprendizado por refor√ßo com arquitetura Actor-Learner.
Os epis√≥dios s√£o gerados em paralelo, experi√™ncias s√£o enviadas para o Learner, e os resultados s√£o logados para an√°lise futura.